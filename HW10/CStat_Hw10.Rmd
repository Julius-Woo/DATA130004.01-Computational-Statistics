---
title: "Computational Statistics HW10"
author: "吴嘉骜 21307130203"
date: "2023-12-12"
output:
  html_document:
    self_contained: yes
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
      number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{=html}
<style type="text/css">
h1.title{
  font-size: 38px;
  color: DarkRed;
  text-align: center;
}
h4.author{
  font-size: 18px;
  color: DarkRed;
  text-align: center;
}
h4.date {
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, cache = TRUE, message = TRUE, fig.align = "center")
```
## ex 9.6
Rao presented an example on genetic linkage of 197 animals in four categories. The group sizes are (125, 18, 20, 34). Assume that the probabilities of the corresponding multinomial distribution are
$$
\left (\frac{1}{2}+\frac{\theta}{4}, \frac{1-\theta}{4},\frac{1-\theta}{4}, \frac{\theta}{4}\right )
$$
Estimate the posterior distribution of $\theta$ given the observed sample, using one of the methods in this chapter.

Solution.

Similar to example 9.5 in the textbook, we can use the random walk Metropolis sampler with a uniform proposal distribution to generate the posterior distribution of $\theta$. Given the observed sample $(x_1, x_2, x_3, x_4)$, the posterior distribution of $\theta$ is
$$
f(\theta|x_1, x_2, x_3, x_4) \propto (2+\theta)^{x_1}(1-\theta)^{x_2+x_3}\theta^{x_4}
$$

```{r}
obs <- c(125, 18, 20, 34)
post <- function(theta){
  if (theta > 0 && theta < 1)
  (2+theta)^obs[1]*(1-theta)^(obs[2]+obs[3])*theta^obs[4]
  else 0
}
m <- 3000
burn <- 600
w <- .1
x <- numeric(m)
set.seed(124)
x[1] <- .1

u <- runif(m)  # for acceptance probability
v <- runif(m, -w, w)  # for proposal distribution
for (i in 2:m){
  y <- x[i-1] + v[i]
  if (u[i] <= post(y)/post(x[i-1])){
    x[i] <- y
  } else {
    x[i] <- x[i-1]
  }
}
xb <- x[(burn+1):m]
theta.hat <- mean(xb)
theta.hat
p <- c(0.5 + theta.hat/4, (1 - theta.hat)/4, (1 - theta.hat)/4,
theta.hat/4)
cat("Estimates:",p*sum(obs))

hist(xb, prob = TRUE, breaks = "Scott", xlab = "theta",
ylab = "x", main = "Generated posterior distribution of theta")
```


## ex 9.7
Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y = \beta_0 + \beta_1X$ to the sample and check the residuals of the model for normality and constant variance.

Solution.

```{r}
# initialize constants and parameters
N <- 5000
burn <- 1000
X <- matrix(0, N, 2)  # the chain
rho <- .9 # correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2

# generate the chain
X[1, ] <- c(mu1, mu2)
for (i in 2:N) {
  x2 <- X[i-1, 2]
  m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
  X[i, 1] <- rnorm(1, m1, s1)
  x1 <- X[i, 1]
  m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
  X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x <- X[b:N, ]
X <- x[, 1]
Y <- x[, 2]

# plot the chain
plot(X, Y, cex=.25, main = "Generated bivariate normal chain")
abline(h = 0, v = 0)

# linear regression
lr <- lm(Y ~ X)
lr

# residual analysis
plot(lr$fit, lr$residuals, cex=.25, main = "Residuals vs Fitted Values")
abline(h = 0)
qqnorm(lr$residuals, cex=.25)
qqline(lr$residuals)
```

The scatter plot of the chain matches the strong positive correlation of the bivariate normal distribution, and the coefficient of the linear regression model is also close to the correlation.

The residuals of the model are approximately normally distributed and have constant variance, as shown in the residual plot and QQ plot.

## ex 9.8
Consider the bivariate density
$$
f(x, y) \propto \binom{n}{x} y^{x+a-1}(1-y)^{n-x+b-1}, \quad  x=0,1,\ldots,n, \ 0 \leq y \leq 1.
$$
It can be shown that for fixed $a, b, n$, the conditional distributions are Binomial$(n, y)$ and Beta$(x + a, n − x + b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.

Solution.

To check the correctness of the Gibbs sampler, we can compare the $x$'s of the generated chain with the marginal target distribution. 
The marginal probability mass function of $x$ is
$$
f(x) \propto \binom{n}{x} \int_0^1 y^{x+a-1}(1-y)^{n-x+b-1} dy = \binom{n}{x} B(x+a, n-x+b),
$$
where $B$ is the beta function. The true mass function is then
$$
f(x) = \binom{n}{x} \frac{B(x+a, n-x+b)}{B(a, b)}.
$$

```{r}
N <- 5000
burn <- 1000
a <- 2
b <- 5
n <- 15
x <- y <- numeric(N)
x[1] <- rbinom(1, n, .5)
y[1] <- rbeta(1, x[1] + a, n - x[1] + b)

for(i in 2:N){
  x[i] <- rbinom(1, n, y[i-1])
  y[i] <- rbeta(1, x[i] + a, n - x[i] + b)
}
xb <- x[(burn+1):N]
est <- table(xb)/length(xb)

# compare with the true distribution
i <- 0:n
fx <-  choose(n, i) * beta(i + a, n - i + b) / beta(a, b)
round(rbind(est, fx), 3)
barplot(rbind(est, fx), beside = TRUE, names.arg = i, legend.text = c("Gibbs", "True"), main = "Comparison of Gibbs sampler and true distribution")
```


## ex 9.10
Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R}$ < 1.2. (See Exercise 9.9.) Also use the $\texttt{coda}$  package to check for convergence of the chain by the Gelman-Rubin
method.

Solution.

```{r}
# Gelman-Rubin method
Gelman.Rubin <- function(psi) {
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)
  B <- n * var(psi.means)  # between-chain variance estimate
  psi.w <- apply(psi, 1, "var")  # within-chain variances
  W <- mean(psi.w)  # within-chain variance estimate
  v.hat <- W*(n-1)/n + (B/n)
  r.hat <- v.hat / W
  return(r.hat)
}

# Rayleigh density
f <- function(x, sigma) {
  if (any(x < 0)) return (0)
  stopifnot(sigma > 0)
  return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

# generate one chain for a Rayleigh distribution
set.seed(1234)
raychain <- function(sigma, N, X1) {
  x <- rep(0, N)
  x[1] <- X1
  u <- runif(N)
  for (i in 2:N) {
    xt <- x[i-1]
    y <- rchisq(1, df=xt)
    num <- f(y, sigma) * dchisq(xt, df=y)
    den <- f(xt, sigma) * dchisq(y, df=xt)
    if (u[i] <= num/den) x[i] <- y else
    x[i] <- xt
  }
  return(x)
}

sigma <- 4
k <- 4 # number of chains
m <- 4000 # length of chains
b <- 300 # burn-in length

x0 <- c(0.125, 0.25, 4, 16) # initial values

# generate k chains
X <- matrix(0, nrow=k, ncol=m)
for (i in 1:k) {
  X[i,] <- raychain(sigma, m, x0[i])
}

# check for convergence by Gelman-Rubin method
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi)) psi[i, ] <- psi[i, ]/(1:ncol(psi))
rhat <- Gelman.Rubin(psi)
rhat

# plot the sequence of R-hat statistics
rhat <- rep(0, m)
for (j in (b+1):m)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):m], type="l", xlab="", ylab="Rhat", main="Rhat for sigma=4")
abline(h=1.2, lty=2)

# use coda package to check for convergence
library(coda)
X1 <- as.mcmc(X[1,])
X2 <- as.mcmc(X[2,])
X3 <- as.mcmc(X[3,])
X4 <- as.mcmc(X[4,])
Y <- mcmc.list(X1, X2, X3, X4)
gelman.diag(Y)
gelman.plot(Y)
```


## Question
Explain why Gibbs sampler is a special case of Metropolis-Hastings algorithm.

Solution.

In the Metropolis-Hastings (M-H) framework, a new proposal $y$ is generated from a proposal distribution $g(\cdot|x^{(t-1)})$ and is accepted as $x^{(t)}$ with probability $\alpha(x^{(t-1)}, y)$.

Gibbs sampling streamlines this by updating one component of $\mathbf{x}^{(t)}$ at a time. Specifically, for the $k$-th component, the proposal distribution is chosen as the conditional distribution of the target distribution $f$, namely
$$
f(\cdot | x_1^{(t)}, \ldots, x_{k-1}^{(t)}, x_{k+1}^{(t-1)}, \ldots, x_K^{(t-1)}) \dot= f(\cdot | \mathbf{x}^{(t-1)}_{-k}).
$$

Thus, the ratio becomes
$$
\frac{f(x^{(t)}_k) f(\mathbf{x}^{(t-1)}_{-k} | x^{(t)}_k)}{f(\mathbf{x}^{(t-1)}_{-k})f(x^{(t-1)}_k | \mathbf{x}^{(t-1)}_{-k})} = \frac{f(x^{(t)}_k, \mathbf{x}^{(t-1)}_{-k})}{f(x^{(t)}_k, \mathbf{x}^{(t-1)}_{-k})} = 1.
$$

The acceptance probability is then
$$
\alpha(x^{(t-1)}_k, x^{(t)}_k) = \min\left\{1, \frac{f(x^{(t)}_k) f(\mathbf{x}^{(t-1)}_{-k} | x^{(t)}_k)}{f(\mathbf{x}^{(t-1)}_{-k})f(x^{(t-1)}_k | \mathbf{x}^{(t-1)}_{-k})}\right\} = 1.
$$

Thus, Gibbs sampling can be viewed as executing a sequence of M-H steps where each proposed update is automatically accepted. In conclusion, Gibbs sampling is a special case of Metropolis-Hastings algorithm with a 100\% acceptance rate.