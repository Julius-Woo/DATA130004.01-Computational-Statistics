---
title: "<center>Computational Statistics HW4</center>"
author: "<center>吴嘉骜 21307130203</center>"
date: "<center>2023-10-19</center>"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### ex 5.12

Let $\hat{\theta}_f^{IS}$ be an importance sampling estimator of $\theta = \int g(x)dx$, where the importance function $f$ is a density. Prove that if $g(x)/f(x)$ is bounded, then the variance of $\hat{\theta}_f^{IS}$ is finite.

**Proof**:

First, we have $\theta = \int g(x)dx$ is finite. Then we calculate the variance of $\hat{\theta}_f^{IS}$: \begin{equation*}
\begin{aligned}
Var(\hat{\theta}_f^{IS}) &= Var\left(\frac{1}{m}\sum\limits_{i=1}^m \frac{g(X_i)}{f(X_i)}\right) \\
&= \frac{1}{m^2} Var\left(\sum\limits_{i=1}^m \frac{g(X_i)}{f(X_i)}\right) \\
&= \frac{1}{m} Var\left(\frac{g(X)}{f(X)}\right) \\
&= \frac{1}{m} \left(E\left(\frac{g(X)}{f(X)}\right)^2 - \left[E\left(\frac{g(X)}{f(X)}\right)\right]^2\right) \\
&= \frac{1}{m} \left(\int \frac{g^2(x)}{f^2(x)}f(x)dx - \left[\int \frac{g(x)}{f(x)}f(x)dx\right]^2\right) \\
&= \frac{1}{m} \left(\int \frac{g^2(x)}{f(x)}dx - \theta^2 \right) \\
\end{aligned}
\end{equation*} Since $g(x)/f(x)$ is bounded, there exists $M > 0$ such that $|g(x)/f(x)| \leq M$ for all $x$ on the support of $g$. $f$ is a density, so $f(x) > 0$, and $\int f(x)dx = 1$. Then we have $\frac{g^2(x)}{f^2(x)} \leq M^2$, therefore \begin{equation*}
\begin{aligned}
Var(\hat{\theta}_f^{IS}) &= \frac{1}{m} \left(\int \frac{g^2(x)}{f^2(x)}f(x)dx - \theta^2 \right) \\
&\leq \frac{1}{m} \left(M^2\int f(x)dx - \theta^2 \right) \\
&= \frac{1}{m} \left(M^2 - \theta^2 \right) \\
&< \infty
\end{aligned}
\end{equation*} Hence the variance of $\hat{\theta}_f^{IS}$ is finite.

### ex 5.13

Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are 'close' to \begin{equation*}
g(x) = \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}, \quad x > 1.
\end{equation*} Which of your two importance functions should produce the smaller variance in estimating \begin{equation*}
\int_1^\infty \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx
\end{equation*} by importance sampling? Explain.

**Solution**: First we plot the graph of $g(x)$, and we consider its expression and graph shape to find two importance functions $f_1$ and $f_2$ that are 'close' to $g(x)$.

```{r}
g <- function(x) {
  x^2/sqrt(2*pi)*exp(-x^2/2)
}
x <- seq(1, 10, 0.01)
f1 <- 2*dnorm(x, 1, 1)
f2 <- dgamma(x-1, 2, 2)
plot(x, g(x), type = "l", col = "red", lwd = 2, xlab = "x", ylab = "density",xlim=c(1,8), ylim=c(0,1))
lines(x, f1, col = "blue", lwd = 2)
lines(x, f2, col = "green", lwd = 2)
legend("topright", c("g(x)", "f1(x)", "f2(x)"), col = c("red", "blue", "green"), lwd = 2)
```

The shape of $g(x)$ is like a normal distribution, attaining its maximum at $x = \sqrt{2}$ and decaying fast as $x$ increases to 4.

We can choose $f_1$ as a normal distribution with mean 1 and variance 1, and truncate it to be supported on $(1, \infty)$. So $f_1$ is twice $N(1,1)$ density on $(1, \infty)$. Actually, $x$ is the square root of $\chi^2(1)$ random variable with a translation 1, so it is not hard to generate.

We can also choose $f_2$ as a translated gamma distribution $\Gamma(\alpha, \lambda)$ with $1<\alpha \leq 2$. We may want its peak to be at $x = \frac{\alpha-1}{\lambda} +1 \approx 1.5$, so we choose $\alpha = 2$ and $\lambda = 2$, namely $f_2$ is a translated $\Gamma(2, 2)$ density on $(1, \infty)$.

The graph of $f_1$ and $f_2$ are plotted above.

Now consider $\text{Var}(\hat{\theta}_f^{IS}) = \frac{1}{m} \text{Var} (\frac{g(X)}{f(X)})$. If we want $\text{Var}(\hat{\theta}_f^{IS})$ to be small, then $\frac{g(X)}{f(X)}$ should be closed to constant. We can plot the graph of $\frac{g(x)}{f_1(x)}$ and $\frac{g(x)}{f_2(x)}$ to see which one is flatter.

```{r}
plot(x, g(x)/f1, type = "l", col = "blue", lwd = 2, xlab = "x", ylab = "g(x)/f(x)",xlim=c(1,8), ylim=c(0,1))
lines(x, g(x)/f2, col = "green", lwd = 2)
legend("topright", c("g(x)/f1(x)", "g(x)/f2(x)"), col = c("blue", "green"), lwd = 2)
```

We can see that $\frac{g(x)}{f_1(x)}$ is closer to constant than $\frac{g(x)}{f_2(x)}$, so $f_1$ should produce the smaller variance in estimating $\int_1^\infty \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$ by importance sampling.

### ex 5.14

Obtain a Monte Carlo estimate of \begin{equation*}
\int_1^\infty \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx
\end{equation*} by importance sampling.

**Solution**:

We can use the importance function $f_1$ found in ex 5.13. See the following R code.

```{r}
gx <- function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2)
}

# generate random variables from f1
m <- 100000
x <- sqrt(rchisq(m,1)) + 1
f <- 2*dnorm(x, 1, 1)
g <- gx(x)
is <- mean(g/f)

# compare the numerical integration
acc <- integrate(gx, 1, Inf)$value
c(is, acc)
```

### Proposition 5.3

Suppose $M = mk$ is the number of replicates for an importance sampling estimator $\hat{\theta}^I$, and $\hat{\theta}^{SI}$ is a stratified importance sampling estimator, with estimates $\hat{\theta}_j$ for $\theta_j$ on the individual strata, each with $m$ replicates. If $\text{Var}(\hat{\theta}^I) = \sigma^2/M$ and $\text{Var}(\hat{\theta}_j) = \sigma^2_j/m$, $j = 1, \cdots, k$, then \begin{equation*}
\sigma^2 - k \sum\limits_{j=1}^k \sigma_j^2 \geq 0,
\end{equation*} with equality if and only if $\theta_1 = · · · = \theta_k$. Hence stratification never increases the variance, and there exists a stratification that reduces the variance except when $g(x)$ is constant.

**Proof**:

We first clarify the two-stage sampling procedure. Suppose we want to estimate $\theta = \int g(x)dx$ by importance sampling.

First we choose a random variable $J$ from integers 1 to $k$ with equal probability $\frac{1}{k}$. After we determine $J = j$, we generate a random variable $X^*$ from the density $f_j$, and calculate $Y^* = \frac{g_j(X^*)}{f_j(X^*)}$. The estimator $\hat{\theta}^{I}$ is the average of $Y^*$ over $M$ replicates.

We claim that $X^*$ and $Y^*$ have the same distribution as $X$ and $Y/k$ respectively, where $X$ is generated from $f$ and $Y = \frac{g(X)}{f(X)}$. To see this, we have \begin{equation*}
\begin{aligned}
f_{X^*}(x) &= \sum\limits_{j=1}^k f_{X^*|J=j}(x|J=j)P(J=j) \\
&= \sum\limits_{j=1}^k \frac{1}{k} f_j(x)\\
&= \sum\limits_{j=1}^k \frac{1}{k} \cdot k f(x)I\{a_{j-1} \leq x \leq a_j\}\\
&= f(x)
\end{aligned}
\end{equation*} so $X^*$ has the same distribution $f$ as $X$. Then we can calculate $Y = \frac{g(X^*)}{f(X^*)}$ once $X^*$ is generated through two stages. Given $J = j$, we obtain $X^* \in [a_{j-1}, a_j)$, so $g_j(X^*) = g(X^*)$ and $f_j(X^*) = kf(X^*)$. Then for $Y^*$, we have \begin{equation*}
\begin{aligned}
F_{Y^*}(y) &= P(Y^* \leq y) \\
&= \sum\limits_{j=1}^k P(Y^* \leq y|J=j)P(J=j)\\
&= \sum\limits_{j=1}^k P(\frac{g_j(X^*)}{f_j(X^*)} \leq y|J=j)P(J=j)\\
&= \sum\limits_{j=1}^k P(\frac{g(X^*)}{kf(X^*)} \leq y|J=j)P(J=j)\\
&= \sum\limits_{j=1}^k P(\frac{Y}{k} \leq y|J=j)P(J=j)\\
&= P(\frac{Y}{k} \leq y)\\
&= F_{Y/k}(y)
\end{aligned}
\end{equation*} so $Y^*$ has the same distribution as $Y/k$.

Now we prove the proposition. We have \begin{equation*}
\begin{aligned}
\sigma^2 &= \text{Var}(Y) = \text{Var}(kY^*) = k^2\text{Var}(Y^*)\\
&= k^2(\text{E}[\text{Var}(Y^*|J)] + \text{Var}(\text{E}[Y^*|J]))\\
&= k^2(\sum\limits_{j=1}^k \sigma_j^2P(J=j) + \text{Var}(\theta_J))\\
&= k^2(\sum\limits_{j=1}^k \sigma_j^2\frac{1}{k} + \text{Var}(\theta_J))\\
&= k\sum\limits_{j=1}^k \sigma_j^2 + k\text{Var}(\theta_J)\\
\end{aligned}
\end{equation*}

Therefore $\sigma^2 - k \sum\limits_{j=1}^k \sigma_j^2 \geq 0$ holds with equality if and only if $\text{Var}(\theta_J) = 0$, which means $\theta_1 = · · · = \theta_k$, or $g(x)$ is constant.
