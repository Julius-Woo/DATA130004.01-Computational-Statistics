---
title: "Computational Statistics HW6"
author: "吴嘉骜 21307130203"
date: "2023-11-15"
output:
  html_document:
    self_contained: yes
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
      number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{=html}
<style type="text/css">
h1.title{
  font-size: 38px;
  color: DarkRed;
  text-align: center;
}
h4.author{
  font-size: 18px;
  color: DarkRed;
  text-align: center;
}
h4.date {
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
  text-align: center;
}

</style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, cache = TRUE, message = TRUE, fig.align = "center")
```
## ex 7.1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

Solution.

```{r}
library(bootstrap)
thetahat <- cor(law$LSAT, law$GPA)
n <- nrow(law)
thetajack <- numeric(n)
for (i in 1:n) {
  thetajack[i] <- cor(law$LSAT[-i], law$GPA[-i])
}
biasjack <- (n-1)*(mean(thetajack)- thetahat)
sejack <- sqrt((n-1)*mean((thetajack-mean(thetajack))^2))
print(list(estimator=thetahat, bias=biasjack, se=sejack))
```

## ex 7.4
Refer to the air-conditioning data set $\texttt{aircondit}$ provided in the $\texttt{boot}$ package.
The 12 observations are the times in hours between failures of air-
conditioning equipment:
$$3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.$$
Assume that the times between failures follow an exponential model Exp$(\lambda)$.
Obtain the MLE of the hazard rate $\lambda$ and use bootstrap to estimate the bias and standard error of the estimate.

Solution.

For exponential distribution, the density function is $f(x)=\lambda e^{-\lambda x}$, and the likelihood function for an i.i.d sample $x_1, \ldots, x_n$ is
$$L(\lambda)=\lambda^n e^{-\lambda \sum_{i=1}^n x_i}.$$
Take the first derivative of $L(\lambda)$, we have
$$\frac{\partial L(\lambda)}{\partial \lambda}=\lambda^{n-1} e^{-\lambda \sum_{i=1}^n x_i} \left \{n-\lambda \sum_{i=1}^n x_i \right \}.$$
Let $\frac{\partial L(\lambda)}{\partial \lambda}=0$, we have $\hat{\lambda}=\frac{n}{\sum_{i=1}^n x_i} = \frac{1}{\bar{x}}$, which is the MLE of $\lambda$.

```{r}
library(boot)
lambdaMLE <- 1/mean(aircondit[,])
B <- 200  # number of bootstrap replicates
n <- nrow(aircondit)  # sample size
lambdaBoot <- numeric(B)
# Bootstrap
for (b in 1:B){
  index <- sample(1:n, size = n, replace = TRUE)
  lambdaBoot[b] <- 1/mean(aircondit[index, ])
}
biasBoot <- mean(lambdaBoot) - lambdaMLE
seBoot <- sd(lambdaBoot)
print(list(MLE=lambdaMLE, bias=biasBoot, se=seBoot))
```

## ex 7.6
Efron and Tibshirani discuss the $\texttt{scor (bootstrap)}$ test score data on 88 students who took examinations in five subjects.
The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book.
Each row of the data frame is a set of scores $(x_{i1}, . . . , x_{i5})$ for the $i^{th}$ student. Use a panel display
to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of the standard errors for each of the following estimates: $\hat{\rho}_{12} = \hat{\rho}$(mec, vec), $\hat{\rho}_{34} = \hat{\rho}$(alg, ana), $\hat{\rho}_{35} = \hat{\rho}$(alg, sta), $\hat{\rho}_{45} = \hat{\rho}$(ana, sta).

Solution.

```{r}
library(bootstrap)
# display
pairs(scor)
# correlation matrix
cor(scor)
```
From the scatter plots and correlation matrix, we can see that all test scores are positively correlated. Among them, the correlation between open book scores (alg, ana and sta) is higher than that between closed book scores (mec and vec). From the correlation matrix, we also find that the correlation between algebra and analysis is the highest, and the correlation between mechanics and statistics is the lowest. The correlation matrix is consistent with the scatter plots.

The Bootstap estimates of the standard errors are as follows:

```{r}
# Bootstrap estimates of se
B <- 200  # number of bootstrap replicates
n <- 88  # sample size
x <- as.matrix(scor)
seBoot <- function(x2col){
  rhoBoot <- numeric(B)
  for(b in 1:B){
  index <- sample(1:n, size = n, replace = TRUE)
  rhoBoot[b] <- cor(x2col[index,1], x2col[index,2])
  }
  sd(rhoBoot)
}

rho12seBoot <- seBoot(x[,1:2])
rho34seBoot <- seBoot(x[,3:4])
rho35seBoot <- seBoot(x[,c(3,5)])
rho45seBoot <- seBoot(x[,4:5])

seMatrix <- matrix(c(rho12seBoot, rho34seBoot, rho35seBoot, rho45seBoot), nrow = 1)
colnames(seMatrix) <- c("rho12", "rho34", "rho35", "rho45")
rownames(seMatrix) <- "se"
print(seMatrix)
```

## ex 7.7
Refer to Exercise 7.6. Efron and Tibshirani discuss the following example. The five-dimensional scores data have a $5 \times 5$ covariance matrix $\Sigma$, with positive eigenvalues $\lambda_1 > · · · > \lambda_5$. In principal components analysis,
$$\theta= \frac{\lambda_1}{\sum_{j=1}^5 \lambda_j}$$
measures the proportion of variance explained by the first principal component. Let $\hat{\lambda}_1 > · · · > \hat{\lambda}_5$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. Compute the sample estimate
$$\hat{\theta}= \frac{\hat{\lambda}_1}{\sum_{j=1}^5 \hat{\lambda}_j}$$ of $\theta$. Obtain the bootstrap estimate of the standard error of $\hat{\theta}$.

Solution.

```{r}
library(bootstrap)
n <- 88  # sample size
# MLE
x <- as.matrix(scor)
covhat <- cov(x)* (n - 1) / n
lambdahat <- eigen(covhat)$values
thetahat <- lambdahat/sum(lambdahat)
thetahat <- thetahat[1]

# Bootstap estimate
B <- 200  # number of bootstrap replicates
thetahatboot <- numeric(B)
for(i in 1:B){
  index <- sample(1:n, size = n, replace = TRUE)
  xboot <- x[index,]
  covhatboot <- cov(xboot)* (n - 1) / n
  lambdahatboot <- eigen(covhatboot)$values
  thetahatboot[i] <- max(lambdahatboot/sum(lambdahatboot))
}
biasBoot <- mean(thetahatboot) - thetahat
seBoot <- sd(thetahatboot)
print(list(MLE=thetahat, bias=biasBoot, se=seBoot))
```

## ex 7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

Solution.

```{r}
library(bootstrap)
n <- 88  # sample size
# MLE
x <- as.matrix(scor)
covhat <- cov(x)* (n - 1) / n
lambdahat <- eigen(covhat)$values
thetahat <- lambdahat/sum(lambdahat)
thetahat <- thetahat[1]

# Jackknife estimate
thetahatjack <- numeric(n)
for(i in 1:n){
  y <- x[-i,]
  covhatjack <- cov(y)* (n - 2) / (n - 1)
  lambdahatjack <- eigen(covhatjack)$values
  thetahatjack[i] <- max(lambdahatjack/sum(lambdahatjack))
}
biasjack <- (n-1)*(mean(thetahatjack) - thetahat)
sejack <- sqrt((n-1)/n * sum((thetahatjack - mean(thetahatjack))^2))
print(list(MLE=thetahat, bias=biasjack, se=sejack))
```

## Problem
Given a set of numbers $X_1, . . . , X_N$ of size $N$. Denote the sample mean and sample
standard deviation by $\overline{X}$ and $S$, where
$$\overline{X} = \frac{1}{N}\sum\limits_{i=1}^N X_i, \quad S=\left \{\frac{1}{N} \sum\limits_{i=1}^N (X_i-\overline{X})^2 \right \}^{1/2}$$
(a)A sample $x_1, . . . , x_n$ of size $n$ is selected from $X_1, . . . , X_N$ by random sampling with replacement. The standard deviation of the sample average $\overline{x} = \frac{1}{n}\sum\limits_{i=1}^n x_i$
is called $\textit{the standard error}$ of $\bar{x}$, denoted by $\text{se}(\bar{x})$. Show that $\text{se}(\bar{x})=S/\sqrt{n}$.

(b)Suppose $n < N$ and $x_1, . . . , x_n$ is selected by random sampling without replacement, show that
$$\text{se}(\bar{x})=\frac{S}{\sqrt{n}} \left(\frac{N-n}{N-1} \right)^{1/2}.$$

(c)A standard Bootstrap sample is obtained in (a) by further assuming $n = N$. Given $X_1, . . . , X_N$ are distinct with each other, what is the number of distinct bootstrap samples?

Proof.

(a)For sampling with replacement, $x_i$ are independent and identically distributed from discrete uniform distribution $U(X_1, . . . , X_N)$. Then $\text{E}(x_i) = \frac{1}{N}\sum\limits_{i=1}^N X_i = \bar{X}$ and
$\text{Var}(x_i) = \text{E}(x_i - \text{E}(x_i))^2 = \frac{1}{N}\sum\limits_{i=1}^N (X_i-\overline{X})^2 =S^2$.
We have se$(\bar{x}) = \sqrt{\text{Var}(\overline{x})}$, where
$$
\begin{align*}
\text{Var}(\bar{x}) &= \text{Var}\left(\frac{1}{n}\sum_{i=1}^n x_i\right) \\
&= \frac{1}{n^2}\sum_{i=1}^n \text{Var}(x_i) \\
&= \frac{1}{n^2} \cdot n \cdot S^2\\
&= \frac{S^2}{n}.
\end{align*}
$$
Thus, se$(\bar{x}) = \frac{S}{\sqrt{n}}$.

(b)For sampling without replacement, $x_i$ are not independent. 

Define $I_i, i = 1, \ldots, N$ as a indicator variable, where $I_i = 1$ if $X_i$ is selected in the sample and $I_i = 0$ otherwise. 
Then $I_i$ is a Bernoulli random variable with probability $P(I_i=1)=\frac{\binom{N-1}{n-1}}{\binom{N}{n}}=\frac{n}{N}$. We have
$\text{E}(I_i) = \frac{n}{N}$ and $\text{Var}(I_i) = \frac{n}{N}(1-\frac{n}{N})$.

Additionally, for $i \neq j$, $\text{E}(I_iI_j) = P(I_i=1, I_j=1) = \frac{\binom{N-2}{n-2}}{\binom{N}{n}} = \frac{n(n-1)}{N(N-1)}$, since the number of samples containing both $i$ and $j$ is $\binom{N-2}{n-2}$.

Thus, the covariance between $I_i$ and $I_j$ ($i \neq j$) is
$$
\mathrm{Cov}(I_i,I_j)={\rm E}[I_iI_j]-{\rm E}[I_i]{\rm E}[I_j]=\frac{n(n-1)}{N(N-1)}-\left(\frac{n}{N}\right)^2=-\frac{n(N-n)}{N^2(N-1)}
$$

Consider $\bar{x}= \frac{1}{n}\sum\limits_{i=1}^n x_i = \frac{1}{n}\sum\limits_{i=1}^N I_i X_i$. The variance of $\bar{x}$ is

$$
\begin{align*}
\mathrm{Var}(\bar{x})
&= \frac{1}{n^2}\mathrm{Var}(\sum_{i=1}^NI_iX_i)\\
&=\frac{1}{n^2}\left[\sum_{i=1}^N\mathrm{Var}(I_iX_i)+\sum_{i\neq j} \mathrm{Cov}(I_iX_i,I_jX_j)\right]\\
&= \frac{1}{n^2}\left[\sum_{i=1}^N X_i^2\mathrm{Var}(I_i)+\sum_{i\neq j} X_iX_j\mathrm{Cov}(I_i,I_j)\right]\\
&= \frac{1}{n^2}\left[\sum_{i=1}^N X_i^2\frac{n}{N}\left(1-\frac{n}{N}\right)-\sum_{i\neq j} X_iX_j\frac{n(N-n)}{N^2(N-1)}\right]\\
&= \frac{1}{nN^2}\frac{N-n}{N-1}\left[(N-1)\sum_{i=1}^N X_i^2-\sum_{i\neq j} X_iX_j\right]\\
&= \frac{1}{nN}\frac{N-n}{N-1}\left[(1-\frac{1}{N})\sum_{i=1}^N X_i^2-\frac{1}{N}\sum_{i\neq j} X_iX_j\right]\\
&= \frac{1}{nN}\frac{N-n}{N-1}\left[\sum_{i=1}^N X_i^2-\frac{1}{N}\left (\sum_{i=1}^N X_i^2+\sum_{i\neq j} X_iX_j\right)\right]\\
&= \frac{1}{nN}\frac{N-n}{N-1}\left[\sum_{i=1}^N X_i^2-\frac{1}{N}\left( N \overline{X}\right)^2\right]\\
&= \frac{1}{nN}\frac{N-n}{N-1}\sum_{i=1}^N \left(X_i-\overline{X}\right)^2\\
&= \frac{1}{n}\frac{N-n}{N-1}S^2\\
\end{align*}
$$

Thus, the standard error of $\overline{x}$ is:

$$
\begin{align*}
\text{se}(\overline{x}) &= \sqrt{\text{Var}(\overline{x})} \\
&= \sqrt{\frac{S^2}{n} \cdot \frac{N-n}{N-1}} \\
&= \frac{S}{\sqrt{n}} \left(\frac{N-n}{N-1} \right)^{1/2}.
\end{align*}
$$

(c)For a standard Bootstrap sampling with replacement, every $x_i$ is selected with probability $1/N$ from $X_1, \ldots, X_N$. Since in general we consider $x_1, \ldots, x_N$ as a permutation rather than combination when drawing the Bootstrap sample, the number of distinct Bootstrap samples is $N^N$.

(If we define two Bootstrap samples, $x_1, \ldots, x_N$, as equivalent whenever they have the same set of distinct values along with their respective frequencies, then the count of distinct Bootstrap samples can be expressed as $\binom{N+N-1}{N} = \binom{2N-1}{N}$ using the combination with replacement formula.)
